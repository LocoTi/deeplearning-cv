{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AdvanceIDE\\Anaconda3\\envs\\mlenv\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "num_classes = 10\n",
    "num_features = 32*32\n",
    "\n",
    "grayscale = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape:  torch.Size([128, 1, 32, 32])\n",
      "labels shape:  torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "resize_transform = transforms.Compose([transforms.Resize((32, 32)),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(\"D:/work/data/Python/mnist/\",\n",
    "                               train=True,\n",
    "                               transform=resize_transform,\n",
    "                               download=True)\n",
    "test_dataset = datasets.MNIST(\"D:/work/data/Python/mnist/\",\n",
    "                              train=False,\n",
    "                              transform=resize_transform,\n",
    "                              download=False)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(\"images shape: \", images.size())\n",
    "    print(\"labels shape: \", labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 1 | Batch index: 0 | Batch size: 128\n",
      "Epoch: 2 | Batch index: 0 | Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "for epoch in range(2):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print(' | Batch index:', batch_idx, end='')\n",
    "        print(' | Batch size:', y.size()[0])\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes, grayscale=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.grayscale = grayscale\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        if self.grayscale:\n",
    "            in_channels = 1\n",
    "        else:\n",
    "            in_channels = 3\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 6, kernel_size=(5, 5), stride=(1, 1)),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=(5, 5)),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        \n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "\n",
    "model = LeNet5(num_classes, grayscale)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "a = torch.randn(3, 4)\n",
    "torch.max(a, dim=1, keepdim=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0049/0469 | Cost: 0.6444\n",
      "Epoch: 001/010 | Batch 0099/0469 | Cost: 0.2702\n",
      "Epoch: 001/010 | Batch 0149/0469 | Cost: 0.2010\n",
      "Epoch: 001/010 | Batch 0199/0469 | Cost: 0.2603\n",
      "Epoch: 001/010 | Batch 0249/0469 | Cost: 0.1589\n",
      "Epoch: 001/010 | Batch 0299/0469 | Cost: 0.1135\n",
      "Epoch: 001/010 | Batch 0349/0469 | Cost: 0.0882\n",
      "Epoch: 001/010 | Batch 0399/0469 | Cost: 0.1276\n",
      "Epoch: 001/010 | Batch 0449/0469 | Cost: 0.0613\n",
      "Epoch: 002/010 | Batch 0049/0469 | Cost: 0.1042\n",
      "Epoch: 002/010 | Batch 0099/0469 | Cost: 0.0210\n",
      "Epoch: 002/010 | Batch 0149/0469 | Cost: 0.0292\n",
      "Epoch: 002/010 | Batch 0199/0469 | Cost: 0.1092\n",
      "Epoch: 002/010 | Batch 0249/0469 | Cost: 0.0227\n",
      "Epoch: 002/010 | Batch 0299/0469 | Cost: 0.0551\n",
      "Epoch: 002/010 | Batch 0349/0469 | Cost: 0.0522\n",
      "Epoch: 002/010 | Batch 0399/0469 | Cost: 0.0267\n",
      "Epoch: 002/010 | Batch 0449/0469 | Cost: 0.1903\n",
      "Epoch: 003/010 | Batch 0049/0469 | Cost: 0.0329\n",
      "Epoch: 003/010 | Batch 0099/0469 | Cost: 0.0459\n",
      "Epoch: 003/010 | Batch 0149/0469 | Cost: 0.0452\n",
      "Epoch: 003/010 | Batch 0199/0469 | Cost: 0.0101\n",
      "Epoch: 003/010 | Batch 0249/0469 | Cost: 0.0685\n",
      "Epoch: 003/010 | Batch 0299/0469 | Cost: 0.0508\n",
      "Epoch: 003/010 | Batch 0349/0469 | Cost: 0.0653\n",
      "Epoch: 003/010 | Batch 0399/0469 | Cost: 0.0194\n",
      "Epoch: 003/010 | Batch 0449/0469 | Cost: 0.0116\n",
      "Epoch: 004/010 | Batch 0049/0469 | Cost: 0.0075\n",
      "Epoch: 004/010 | Batch 0099/0469 | Cost: 0.0091\n",
      "Epoch: 004/010 | Batch 0149/0469 | Cost: 0.0308\n",
      "Epoch: 004/010 | Batch 0199/0469 | Cost: 0.0694\n",
      "Epoch: 004/010 | Batch 0249/0469 | Cost: 0.0577\n",
      "Epoch: 004/010 | Batch 0299/0469 | Cost: 0.0311\n",
      "Epoch: 004/010 | Batch 0349/0469 | Cost: 0.0434\n",
      "Epoch: 004/010 | Batch 0399/0469 | Cost: 0.0359\n",
      "Epoch: 004/010 | Batch 0449/0469 | Cost: 0.0610\n",
      "Epoch: 005/010 | Batch 0049/0469 | Cost: 0.0264\n",
      "Epoch: 005/010 | Batch 0099/0469 | Cost: 0.0266\n",
      "Epoch: 005/010 | Batch 0149/0469 | Cost: 0.0068\n",
      "Epoch: 005/010 | Batch 0199/0469 | Cost: 0.0333\n",
      "Epoch: 005/010 | Batch 0249/0469 | Cost: 0.0369\n",
      "Epoch: 005/010 | Batch 0299/0469 | Cost: 0.0107\n",
      "Epoch: 005/010 | Batch 0349/0469 | Cost: 0.0956\n",
      "Epoch: 005/010 | Batch 0399/0469 | Cost: 0.0271\n",
      "Epoch: 005/010 | Batch 0449/0469 | Cost: 0.0091\n",
      "Epoch: 006/010 | Batch 0049/0469 | Cost: 0.0255\n",
      "Epoch: 006/010 | Batch 0099/0469 | Cost: 0.0228\n",
      "Epoch: 006/010 | Batch 0149/0469 | Cost: 0.0181\n",
      "Epoch: 006/010 | Batch 0199/0469 | Cost: 0.0259\n",
      "Epoch: 006/010 | Batch 0249/0469 | Cost: 0.0199\n",
      "Epoch: 006/010 | Batch 0299/0469 | Cost: 0.0263\n",
      "Epoch: 006/010 | Batch 0349/0469 | Cost: 0.0146\n",
      "Epoch: 006/010 | Batch 0399/0469 | Cost: 0.0126\n",
      "Epoch: 006/010 | Batch 0449/0469 | Cost: 0.0036\n",
      "Epoch: 007/010 | Batch 0049/0469 | Cost: 0.0368\n",
      "Epoch: 007/010 | Batch 0099/0469 | Cost: 0.0230\n",
      "Epoch: 007/010 | Batch 0149/0469 | Cost: 0.0091\n",
      "Epoch: 007/010 | Batch 0199/0469 | Cost: 0.0011\n",
      "Epoch: 007/010 | Batch 0249/0469 | Cost: 0.0180\n",
      "Epoch: 007/010 | Batch 0299/0469 | Cost: 0.0314\n",
      "Epoch: 007/010 | Batch 0349/0469 | Cost: 0.0150\n",
      "Epoch: 007/010 | Batch 0399/0469 | Cost: 0.0172\n",
      "Epoch: 007/010 | Batch 0449/0469 | Cost: 0.0254\n",
      "Epoch: 008/010 | Batch 0049/0469 | Cost: 0.0256\n",
      "Epoch: 008/010 | Batch 0099/0469 | Cost: 0.0125\n",
      "Epoch: 008/010 | Batch 0149/0469 | Cost: 0.0106\n",
      "Epoch: 008/010 | Batch 0199/0469 | Cost: 0.0498\n",
      "Epoch: 008/010 | Batch 0249/0469 | Cost: 0.0005\n",
      "Epoch: 008/010 | Batch 0299/0469 | Cost: 0.0280\n",
      "Epoch: 008/010 | Batch 0349/0469 | Cost: 0.0063\n",
      "Epoch: 008/010 | Batch 0399/0469 | Cost: 0.0306\n",
      "Epoch: 008/010 | Batch 0449/0469 | Cost: 0.0155\n",
      "Epoch: 009/010 | Batch 0049/0469 | Cost: 0.0021\n",
      "Epoch: 009/010 | Batch 0099/0469 | Cost: 0.0061\n",
      "Epoch: 009/010 | Batch 0149/0469 | Cost: 0.0039\n",
      "Epoch: 009/010 | Batch 0199/0469 | Cost: 0.0139\n",
      "Epoch: 009/010 | Batch 0249/0469 | Cost: 0.0192\n",
      "Epoch: 009/010 | Batch 0299/0469 | Cost: 0.0032\n",
      "Epoch: 009/010 | Batch 0349/0469 | Cost: 0.0075\n",
      "Epoch: 009/010 | Batch 0399/0469 | Cost: 0.0017\n",
      "Epoch: 009/010 | Batch 0449/0469 | Cost: 0.0354\n",
      "Epoch: 010/010 | Batch 0049/0469 | Cost: 0.0085\n",
      "Epoch: 010/010 | Batch 0099/0469 | Cost: 0.0024\n",
      "Epoch: 010/010 | Batch 0149/0469 | Cost: 0.0029\n",
      "Epoch: 010/010 | Batch 0199/0469 | Cost: 0.0009\n",
      "Epoch: 010/010 | Batch 0249/0469 | Cost: 0.0537\n",
      "Epoch: 010/010 | Batch 0299/0469 | Cost: 0.0027\n",
      "Epoch: 010/010 | Batch 0349/0469 | Cost: 0.0071\n",
      "Epoch: 010/010 | Batch 0399/0469 | Cost: 0.0112\n",
      "Epoch: 010/010 | Batch 0449/0469 | Cost: 0.0009\n",
      "Epoch: 010/010 | Train: 99.880%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, dataloader):\n",
    "    correct, num_samples = 0, 0\n",
    "    for i, (features, targets) in enumerate(dataloader):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        logits, probas = model(features)\n",
    "        _, predict_labels = torch.max(probas, 1)\n",
    "        \n",
    "        num_samples += targets.size(0)\n",
    "        correct += (predict_labels == targets).sum()\n",
    "    \n",
    "    return correct.float() / num_samples * 100\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        # backward\n",
    "        cost.backward()\n",
    "        \n",
    "        # update\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "model.eval()\n",
    "with torch.no_grad(): # save memory during inference\n",
    "    print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "          epoch+1, epochs, \n",
    "          accuracy(model, train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 32, 32])\n",
      "torch.Size([128])\n",
      "torch.Size([32, 32, 1])\n",
      "(32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx, (features, targets) in enumerate(test_loader):\n",
    "    features = features\n",
    "    targets = targets\n",
    "    break\n",
    "\n",
    "print(features.size())\n",
    "print(targets.size())\n",
    "nhwc_img = np.transpose(features[0], axes=(1, 2, 0))\n",
    "print(nhwc_img.size())\n",
    "nhw_img = np.squeeze(nhwc_img.numpy(), axis=2)\n",
    "print(nhw_img.shape)\n",
    "plt.imshow(nhw_img, cmap='Greys')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
