{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:\n",
    "\n",
    "https://github.com/lukemelas/EfficientNet-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch import Tensor\n",
    "from typing import Optional, Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\") # to include ../helper_utils.py etc.\n",
    "from helper_utils import set_all_seeds, set_deterministic\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "learning_rate = 0.0001\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "save_path = \"efficientnet.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(random_seed)\n",
    "\n",
    "set_deterministic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 dataloader workers every process\n",
      "train images num:  3306\n",
      "test images num:  364\n",
      "images shape:  torch.Size([8, 3, 224, 224])\n",
      "labels shape:  torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "img_size = {\"B0\": 224,\n",
    "            \"B1\": 240,\n",
    "            \"B2\": 260,\n",
    "            \"B3\": 300,\n",
    "            \"B4\": 380,\n",
    "            \"B5\": 456,\n",
    "            \"B6\": 528,\n",
    "            \"B7\": 600}\n",
    "num_model = \"B0\"\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(img_size[num_model]),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "test_transform = transforms.Compose([transforms.RandomResizedCrop((img_size[num_model], img_size[num_model])),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "image_path = \"D:/work/data/Python/flower_data/\"\n",
    "assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"), transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"), transform=test_transform)\n",
    "\n",
    "flower_list = train_dataset.class_to_idx\n",
    "class_dict = dict((val, key) for key, val in flower_list.items())\n",
    "\n",
    "# dump dict too json file\n",
    "json_str = json.dumps(class_dict, indent=4)\n",
    "with open(\"class_indices.json\", \"w\") as json_file:\n",
    "    json_file.write(json_str)\n",
    "#     json.dump(class_dict, json_file, ensure_ascii=False)\n",
    "\n",
    "num_workers = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 0])\n",
    "print('Using {} dataloader workers every process'.format(num_workers))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "train_num = len(train_dataset)\n",
    "test_num = len(test_dataset)\n",
    "\n",
    "print(\"train images num: \", train_num)\n",
    "print(\"test images num: \", test_num)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(\"images shape: \", images.size())\n",
    "    print(\"labels shape: \", labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model\n",
    "\n",
    "![EfficientNet](https://img-blog.csdnimg.cn/20210306135337653.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNet几个注意点：\n",
    "\n",
    "MBConv块类似与MobileNetV3网络的倒残差块，但有些许区别：\n",
    "\n",
    "结构如下：\n",
    "\n",
    "![MBConv块](https://img-blog.csdnimg.cn/20210419135003777.png#pic_center)\n",
    "```\n",
    "input \n",
    "-> Conv(k=1x1,stride=s1) + BN + Swish \n",
    "-> Depthwise Conv(k=kxk,stride=s1/s2) + BN + Swish \n",
    "-> SE \n",
    "-> Conv(k=1x1,stride=s1) + BN \n",
    "-> Dropout \n",
    "-> out\n",
    "```\n",
    "\n",
    "- 第一个升维的1x1卷积层，它的卷积核个数是输入特征矩阵channel的n nn倍，n = {1或6}。\n",
    "- n=1时，不要第一个升维的1x1卷积层，即Stage2中的MBConv结构没有第一个升维的1x1卷积层\n",
    "- 关于shortcut连接，仅当输入MBConv结构的特征矩阵与输出的特征矩阵shape相同时才存在（代码中可通过stride==1 and inputc_channels==output_channels条件来判断）。\n",
    "- SE模块如下所示，由一个全局平均池化，两个全连接层组成。第一个全连接层的节点个数是输入该MBConv特征矩阵channels的$1/4$，且使用Swish激活函数。第二个全连接层的节点个数等于Depthwise Conv层输出的特征矩阵channels，且使用Sigmoid激活函数。\n",
    "- Dropout层的dropout_rate在tensorflow的keras源码中对应的是drop_connect_rate后面会细讲（注意，在源码实现中只有使用shortcut的时候才有Dropout层）。\n",
    "\n",
    "![SE模块](https://img-blog.csdnimg.cn/20210306151615976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQxMDk3,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu(input):\n",
    "    '''\n",
    "    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n",
    "        SiLU(x) = x * sigmoid(x)\n",
    "    '''\n",
    "    return input * torch.sigmoid(\n",
    "        input)  # use torch.sigmoid to make sure that we created the most efficient implemetation based on builtin PyTorch functions\n",
    "\n",
    "\n",
    "# create a class wrapper from PyTorch nn.Module, so\n",
    "# the function now can be easily used in models\n",
    "class SiLU(nn.Module):\n",
    "    '''\n",
    "    Applies the Sigmoid Linear Unit (SiLU) function element-wise:\n",
    "        SiLU(x) = x * sigmoid(x)\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    References:\n",
    "        -  Related paper:\n",
    "        https://arxiv.org/pdf/1606.08415.pdf\n",
    "    Examples:\n",
    "        >>> m = silu()\n",
    "        >>> input = torch.randn(2)\n",
    "        >>> output = m(input)\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Init method.\n",
    "        '''\n",
    "        super().__init__()  # init the base class\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        '''\n",
    "        return silu(input)  # simply apply already implemented SiLU\n",
    "\n",
    "\n",
    "def _make_divisible(ch, divisor=8, min_ch=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_ch is None:\n",
    "        min_ch = divisor\n",
    "    new_ch = max(min_ch, int(ch + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_ch < 0.9 * ch:\n",
    "        new_ch += divisor\n",
    "    return new_ch\n",
    "\n",
    "# Conv1x1 + BN + Activation(Swish) 模块\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 in_planes: int,\n",
    "                 out_planes: int,\n",
    "                 kernel_size: int = 3,\n",
    "                 stride: int = 1,\n",
    "                 groups: int = 1,\n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 activation_layer: Optional[Callable[..., nn.Module]] = None):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if activation_layer is None:\n",
    "            activation_layer = SiLU  # alias Swish  (torch>=1.7)\n",
    "\n",
    "        super(ConvBNActivation, self).__init__(nn.Conv2d(in_channels=in_planes,\n",
    "                                                         out_channels=out_planes,\n",
    "                                                         kernel_size=kernel_size,\n",
    "                                                         stride=stride,\n",
    "                                                         padding=padding,\n",
    "                                                         groups=groups,\n",
    "                                                         bias=False),\n",
    "                                               norm_layer(out_planes),\n",
    "                                               activation_layer())\n",
    "\n",
    "# SE 模块\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_c: int,   # block input channel\n",
    "                 expand_c: int,  # block expand channel\n",
    "                 squeeze_factor: int = 4):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        squeeze_c = input_c // squeeze_factor\n",
    "        self.fc1 = nn.Conv2d(expand_c, squeeze_c, 1)\n",
    "        self.ac1 = SiLU()  # alias Swish\n",
    "        self.fc2 = nn.Conv2d(squeeze_c, expand_c, 1)\n",
    "        self.ac2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        scale = F.adaptive_avg_pool2d(x, output_size=(1, 1))\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.ac1(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        scale = self.ac2(scale)\n",
    "        return scale * x\n",
    "\n",
    "\n",
    "# MBConv 模块\n",
    "class InvertedResidualConfig:\n",
    "    # kernel_size, in_channel, out_channel, exp_ratio, strides, use_SE, drop_connect_rate\n",
    "    def __init__(self,\n",
    "                 kernel: int,          # 3 or 5\n",
    "                 input_c: int,\n",
    "                 out_c: int,\n",
    "                 expanded_ratio: int,  # 1 or 6\n",
    "                 stride: int,          # 1 or 2\n",
    "                 use_se: bool,         # True\n",
    "                 drop_rate: float,\n",
    "                 index: str,           # 1a, 2a, 2b, ...\n",
    "                 width_coefficient: float):\n",
    "        self.input_c = self.adjust_channels(input_c, width_coefficient)\n",
    "        self.kernel = kernel\n",
    "        self.expanded_c = self.input_c * expanded_ratio\n",
    "        self.out_c = self.adjust_channels(out_c, width_coefficient)\n",
    "        self.use_se = use_se\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "        self.index = index\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_channels(channels: int, width_coefficient: float):\n",
    "        return _make_divisible(channels * width_coefficient, 8)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cnf: InvertedResidualConfig,\n",
    "                 norm_layer: Callable[..., nn.Module]):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "\n",
    "        if cnf.stride not in [1, 2]:\n",
    "            raise ValueError(\"illegal stride value.\")\n",
    "\n",
    "        self.use_res_connect = (cnf.stride == 1 and cnf.input_c == cnf.out_c)\n",
    "\n",
    "        layers = OrderedDict()\n",
    "        activation_layer = SiLU  # alias Swish\n",
    "\n",
    "        # expand\n",
    "        # 第一个Conv1x1，升维\n",
    "        # input_channel=expanded_channel时，n=1的情况，即Stage2没有第一个Con1x1卷积\n",
    "        if cnf.expanded_c != cnf.input_c:\n",
    "            layers.update({\"expand_conv\": ConvBNActivation(cnf.input_c,\n",
    "                                                           cnf.expanded_c,\n",
    "                                                           kernel_size=1,\n",
    "                                                           norm_layer=norm_layer,\n",
    "                                                           activation_layer=activation_layer)})\n",
    "\n",
    "        # depthwise\n",
    "        layers.update({\"dwconv\": ConvBNActivation(cnf.expanded_c,\n",
    "                                                  cnf.expanded_c,\n",
    "                                                  kernel_size=cnf.kernel,\n",
    "                                                  stride=cnf.stride,\n",
    "                                                  groups=cnf.expanded_c,\n",
    "                                                  norm_layer=norm_layer,\n",
    "                                                  activation_layer=activation_layer)})\n",
    "\n",
    "        if cnf.use_se:\n",
    "            layers.update({\"se\": SqueezeExcitation(cnf.input_c,\n",
    "                                                   cnf.expanded_c)})\n",
    "\n",
    "        # project\n",
    "        # 第二个Conv1x1，降维\n",
    "        layers.update({\"project_conv\": ConvBNActivation(cnf.expanded_c,\n",
    "                                                        cnf.out_c,\n",
    "                                                        kernel_size=1,\n",
    "                                                        norm_layer=norm_layer,\n",
    "                                                        activation_layer=nn.Identity)})\n",
    "\n",
    "        self.block = nn.Sequential(layers)\n",
    "        self.out_channels = cnf.out_c\n",
    "        self.is_strided = cnf.stride > 1\n",
    "\n",
    "        # 只有在使用shortcut连接时才使用dropout层\n",
    "        if self.use_res_connect and cnf.drop_rate > 0:\n",
    "            self.dropout = nn.Dropout2d(p=cnf.drop_rate, inplace=True)\n",
    "        else:\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = self.block(x)\n",
    "        result = self.dropout(result)\n",
    "        if self.use_res_connect:\n",
    "            result += x\n",
    "\n",
    "        return result\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    '''\n",
    "    pramas:\n",
    "        width_coefficient: 增加网络宽度：即增加channel数量\n",
    "        depth_coefficient：增加网络深度：增加网络layer，即增加每个Stage内MBConv重复次数\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 width_coefficient: float,\n",
    "                 depth_coefficient: float,\n",
    "                 num_classes: int = 1000,\n",
    "                 dropout_rate: float = 0.2,\n",
    "                 drop_connect_rate: float = 0.2,\n",
    "                 block: Optional[Callable[..., nn.Module]] = None,\n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "                 ):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        \n",
    "        # kernel_size, in_channel, out_channel, exp_ratio, strides, use_SE, drop_connect_rate, repeats\n",
    "        default_cnf = [[3, 32, 16, 1, 1, True, drop_connect_rate, 1],\n",
    "                       [3, 16, 24, 6, 2, True, drop_connect_rate, 2],\n",
    "                       [5, 24, 40, 6, 2, True, drop_connect_rate, 2],\n",
    "                       [3, 40, 80, 6, 2, True, drop_connect_rate, 3],\n",
    "                       [5, 80, 112, 6, 1, True, drop_connect_rate, 3],\n",
    "                       [5, 112, 192, 6, 2, True, drop_connect_rate, 4],\n",
    "                       [3, 192, 320, 6, 1, True, drop_connect_rate, 1]]\n",
    "        \n",
    "        # 计算MBConv重复次数\n",
    "        def round_repeats(repeats):\n",
    "            \"\"\"Round number of repeats based on depth multiplier.\"\"\"\n",
    "            return int(math.ceil(depth_coefficient * repeats))\n",
    "\n",
    "        if block is None:\n",
    "            block = InvertedResidual\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = partial(nn.BatchNorm2d, eps=1e-3, momentum=0.1)\n",
    "        \n",
    "        # 根据width_coefficient系数得出调整后的channel数\n",
    "        adjust_channels = partial(InvertedResidualConfig.adjust_channels,\n",
    "                                  width_coefficient=width_coefficient)\n",
    "\n",
    "        # build inverted_residual_setting\n",
    "        bneck_conf = partial(InvertedResidualConfig,\n",
    "                             width_coefficient=width_coefficient)\n",
    "\n",
    "        b = 0\n",
    "        # 每个Stage中MBConv的重复次数\n",
    "        num_blocks = float(sum(round_repeats(i[-1]) for i in default_cnf))\n",
    "        inverted_residual_setting = []\n",
    "        for stage, args in enumerate(default_cnf):\n",
    "            cnf = copy.copy(args)\n",
    "            # cnf最后一个值为重复次数，需要弹出，然后传入InvertedResidualConfig\n",
    "            for i in range(round_repeats(cnf.pop(-1))):\n",
    "                # 每个Stage中第一个MBConv的stride为cnf指定的值，其他MBConv的stride都为1\n",
    "                if i > 0:\n",
    "                    # strides equal 1 except first cnf\n",
    "                    cnf[-3] = 1  # strides\n",
    "                    cnf[1] = cnf[2]  # input_channel equal output_channel\n",
    "\n",
    "                cnf[-1] *= b / num_blocks  # update dropout ratio\n",
    "                index = str(stage + 1) + chr(i + 97)  # 1a, 2a, 2b, ...\n",
    "                inverted_residual_setting.append(bneck_conf(*cnf, index))\n",
    "                b += 1\n",
    "\n",
    "        # create layers\n",
    "        layers = OrderedDict()\n",
    "        \n",
    "        # Stage1\n",
    "        # first conv\n",
    "        layers.update({\"stem_conv\": ConvBNActivation(in_planes=3,\n",
    "                                                     out_planes=adjust_channels(32),\n",
    "                                                     kernel_size=3,\n",
    "                                                     stride=2,\n",
    "                                                     norm_layer=norm_layer)})\n",
    "\n",
    "        # Stage2 -> Stage8\n",
    "        # building inverted residual blocks\n",
    "        for cnf in inverted_residual_setting:\n",
    "            layers.update({cnf.index: block(cnf, norm_layer)})\n",
    "\n",
    "        # Stage9\n",
    "        # build top\n",
    "        last_conv_input_c = inverted_residual_setting[-1].out_c\n",
    "        last_conv_output_c = adjust_channels(1280)\n",
    "        layers.update({\"top\": ConvBNActivation(in_planes=last_conv_input_c,\n",
    "                                               out_planes=last_conv_output_c,\n",
    "                                               kernel_size=1,\n",
    "                                               norm_layer=norm_layer)})\n",
    "        \n",
    "        print(layers)\n",
    "        self.features = nn.Sequential(layers)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        classifier = []\n",
    "        if dropout_rate > 0:\n",
    "            classifier.append(nn.Dropout(p=dropout_rate, inplace=True))\n",
    "        classifier.append(nn.Linear(last_conv_output_c, num_classes))\n",
    "        self.classifier = nn.Sequential(*classifier)\n",
    "\n",
    "        # initial weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def efficientnet_b0(num_classes=1000):\n",
    "    # input image size 224x224\n",
    "    return EfficientNet(width_coefficient=1.0,\n",
    "                        depth_coefficient=1.0,\n",
    "                        dropout_rate=0.2,\n",
    "                        num_classes=num_classes)\n",
    "\n",
    "\n",
    "def efficientnet_b1(num_classes=1000):\n",
    "    # input image size 240x240\n",
    "    return EfficientNet(width_coefficient=1.0,\n",
    "                        depth_coefficient=1.1,\n",
    "                        dropout_rate=0.2,\n",
    "                        num_classes=num_classes)\n",
    "\n",
    "\n",
    "def efficientnet_b2(num_classes=1000):\n",
    "    # input image size 260x260\n",
    "    return EfficientNet(width_coefficient=1.1,\n",
    "                        depth_coefficient=1.2,\n",
    "                        dropout_rate=0.3,\n",
    "                        num_classes=num_classes)\n",
    "\n",
    "\n",
    "def efficientnet_b3(num_classes=1000):\n",
    "    # input image size 300x300\n",
    "    return EfficientNet(width_coefficient=1.2,\n",
    "                        depth_coefficient=1.4,\n",
    "                        dropout_rate=0.3,\n",
    "                        num_classes=num_classes)\n",
    "\n",
    "\n",
    "def efficientnet_b4(num_classes=1000):\n",
    "    # input image size 380x380\n",
    "    return EfficientNet(width_coefficient=1.4,\n",
    "                        depth_coefficient=1.8,\n",
    "                        dropout_rate=0.4,\n",
    "                        num_classes=num_classes)\n",
    "\n",
    "\n",
    "def efficientnet_b5(num_classes=1000):\n",
    "    # input image size 456x456\n",
    "    return EfficientNet(width_coefficient=1.6,\n",
    "                        depth_coefficient=2.2,\n",
    "                        dropout_rate=0.4,\n",
    "                        num_classes=num_classes)\n",
    "\n",
    "\n",
    "def efficientnet_b6(num_classes=1000):\n",
    "    # input image size 528x528\n",
    "    return EfficientNet(width_coefficient=1.8,\n",
    "                        depth_coefficient=2.6,\n",
    "                        dropout_rate=0.5,\n",
    "                        num_classes=num_classes)\n",
    "\n",
    "\n",
    "def efficientnet_b7(num_classes=1000):\n",
    "    # input image size 600x600\n",
    "    return EfficientNet(width_coefficient=2.0,\n",
    "                        depth_coefficient=3.1,\n",
    "                        dropout_rate=0.5,\n",
    "                        num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "OrderedDict([('stem_conv', ConvBNActivation(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): SiLU()\n",
      ")), ('1a', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Identity()\n",
      ")), ('2a', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Identity()\n",
      ")), ('2b', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.0015625, inplace=True)\n",
      ")), ('3a', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Identity()\n",
      ")), ('3b', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.009375000000000001, inplace=True)\n",
      ")), ('4a', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Identity()\n",
      ")), ('4b', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.0234375, inplace=True)\n",
      ")), ('4c', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.01025390625, inplace=True)\n",
      ")), ('5a', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Identity()\n",
      ")), ('5b', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.05625, inplace=True)\n",
      ")), ('5c', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.03515625, inplace=True)\n",
      ")), ('6a', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Identity()\n",
      ")), ('6b', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.10312500000000001, inplace=True)\n",
      ")), ('6c', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.08378906250000001, inplace=True)\n",
      ")), ('6d', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.07331542968750002, inplace=True)\n",
      ")), ('7a', InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (expand_conv): ConvBNActivation(\n",
      "      (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (dwconv): ConvBNActivation(\n",
      "      (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (se): SqueezeExcitation(\n",
      "      (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac1): SiLU()\n",
      "      (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (ac2): Sigmoid()\n",
      "    )\n",
      "    (project_conv): ConvBNActivation(\n",
      "      (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Identity()\n",
      ")), ('top', ConvBNActivation(\n",
      "  (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): SiLU()\n",
      "))])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = efficientnet_b0(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "# train_steps: train_num // batch_size\n",
    "train_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_loader)\n",
    "    running_loss = 0.0\n",
    "    for step, (images, labels) in enumerate(train_bar):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        logits = model(images)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # logging\n",
    "        running_loss += loss.item()\n",
    "        train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1, epochs, loss)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader)\n",
    "        for images, labels in test_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(images)\n",
    "            _, predict_labels = torch.max(logits, dim=1)\n",
    "            correct += torch.eq(predict_labels, labels).sum().float()\n",
    "    \n",
    "    test_acc = correct / test_num\n",
    "    train_loss = running_loss / train_steps\n",
    "    print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
    "              (epoch + 1, train_loss, test_acc))\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(\"save model pth to %s\" % (save_path))\n",
    "    \n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "img_path = \"D:/work/data/Python/flower_data/val/tulips/10995953955_089572caf0.jpg\"\n",
    "assert os.path.exists(img_path), \"file: '{}' dose not exist.\".format(img_path)\n",
    "img = Image.open(img_path)\n",
    "\n",
    "print(np.array(img).shape)\n",
    "plt.imshow(img)\n",
    "# [N, C, H, W]\n",
    "img = test_transform(img)\n",
    "print(img.shape)\n",
    "# expand batch dimension\n",
    "img = torch.unsqueeze(img, dim=0)\n",
    "\n",
    "# read class_indict\n",
    "json_path = './class_indices.json'\n",
    "assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\n",
    "\n",
    "json_file = open(json_path, \"r\")\n",
    "class_indict = json.load(json_file)\n",
    "\n",
    "# create model\n",
    "model = efficientnet_b0(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# load model weights\n",
    "weights_path = \"./efficientnet.pth\"\n",
    "assert os.path.exists(weights_path), \"file: '{}' dose not exist.\".format(weights_path)\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img = img.to(device)\n",
    "    # logits shape: (1, 5)\n",
    "    logits = model(img)\n",
    "    logits = torch.squeeze(logits).cpu()\n",
    "    predict = torch.softmax(logits, dim=0)\n",
    "    # if want to use numpy(), the tensor must be on cpu\n",
    "    predict_class = torch.argmax(predict).numpy()\n",
    "\n",
    "print_res = \"class: {}   prob: {:.3}\".format(class_indict[str(predict_class)],\n",
    "                                             predict[predict_class].numpy())\n",
    "plt.title(print_res)\n",
    "print(print_res)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
