{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking an Image Dataset for Minibatch Training using NumPy NPZ Archives\n",
    "This notebook provides an example for how to organize a large dataset of images into chunks for quick access during minibatch learning. This approach uses NumPy .npz archive files and only requires having NumPy as a dependency so that this approach should be compatible with different Python-based machine learning and deep learning libraries and packages for further image (pre)processing and augmentation.\n",
    "\n",
    "While this approach performs reasonably well (sufficiently well for my applications), you may also be interested in TensorFlow's \"Reading Data\" guide to work with TfRecords and file queues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from helper import mnist_export_to_jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "mnist_path = \"D:/work/data/Python/tensorflow/mnist/data/\"\n",
    "mnist_export_to_jpg(path=mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for i in ('train', 'valid', 'test'):\n",
    "    print('mnist_%s subdirectories' % i, os.listdir(os.path.join(mnist_path, 'mnist_%s' % i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "mnist_train_path = os.path.join(mnist_path, 'mnist_train/9/')\n",
    "some_img = os.path.join(mnist_train_path, os.listdir(mnist_train_path)[0])\n",
    "\n",
    "img = mpimg.imread(some_img)\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap='binary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chunking Images into NumPy NPZ Archive Files\n",
    "The following wrapper function creates .npz archive files training, testing, and validation. It will group images together into integer arrays that are then saved as .npz archive files. The number of rows (images) in each .npz archive will be equal to the archive_size argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "def images_to_pickles(data_stempath, which_set='train', \n",
    "                      archive_size=5000, width=28, height=28, channels=1,\n",
    "                      shuffle=False, seed=None):\n",
    "    \n",
    "    \n",
    "    if not os.path.exists('%snpz' % data_stempath):\n",
    "        os.mkdir('%snpz' % data_stempath)\n",
    "        \n",
    "    img_paths = [p for p in glob.iglob('%s/**/*.jpg' % \n",
    "                                   os.path.join(data_stempath, which_set), recursive=True)]\n",
    "    if shuffle:\n",
    "        rgen = np.random.RandomState(seed)\n",
    "        paths = rgen.shuffle(img_paths)\n",
    "    \n",
    "    idx, file_idx = 0, 1\n",
    "    data = np.zeros((archive_size, height, width, channels), dtype=np.uint8)\n",
    "    labels = np.zeros(archive_size, dtype=np.uint8)\n",
    "    for path in img_paths:\n",
    "        if idx >= archive_size - 1:\n",
    "            idx = 0\n",
    "            savepath = os.path.join('%snpz' % data_stempath, '%s_%d.npz' % \n",
    "                                    (which_set, file_idx))\n",
    "            file_idx += 1\n",
    "            np.savez(savepath, data=data, labels=labels)\n",
    "\n",
    "        label = int(os.path.basename(os.path.dirname(path)))\n",
    "        image = mpimg.imread(path)\n",
    "        \n",
    "        if len(image.shape) == 2:\n",
    "            data[idx] = image[:, :, np.newaxis]\n",
    "        labels[idx] = label\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_pickles(data_stempath=mnist_path, which_set='mnist_train', shuffle=True, seed=1)\n",
    "images_to_pickles(data_stempath=mnist_path, which_set='mnist_valid', shuffle=True, seed=1)\n",
    "images_to_pickles(data_stempath=mnist_path, which_set='mnist_test', shuffle=True, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('mnist_npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('mnist_npz/test_1.npz')\n",
    "print(data['data'].shape)\n",
    "print(data['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data['data'][0][:, :, -1], cmap='binary');\n",
    "print('Class label:', data['labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLoader():\n",
    "    def __init__(self, minibatches_path, \n",
    "                 normalize=True):\n",
    "        \n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.train_batchpaths = [os.path.join(minibatches_path, f)\n",
    "                                 for f in os.listdir(minibatches_path)\n",
    "                                 if 'train' in f]\n",
    "        self.valid_batchpaths = [os.path.join(minibatches_path, f)\n",
    "                                 for f in os.listdir(minibatches_path)\n",
    "                                 if 'valid' in f]\n",
    "        self.test_batchpaths = [os.path.join(minibatches_path, f)\n",
    "                                 for f in os.listdir(minibatches_path)\n",
    "                                 if 'train' in f]\n",
    "\n",
    "        self.num_train = 45000\n",
    "        self.num_valid = 5000\n",
    "        self.num_test = 10000\n",
    "        self.n_classes = 10\n",
    "\n",
    "\n",
    "    def load_train_epoch(self, batch_size=50, onehot=False,\n",
    "                         shuffle_within=False, shuffle_paths=False,\n",
    "                         seed=None):\n",
    "        for batch_x, batch_y in self._load_epoch(which='train',\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 onehot=onehot,\n",
    "                                                 shuffle_within=shuffle_within,\n",
    "                                                 shuffle_paths=shuffle_paths,\n",
    "                                                 seed=seed):\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def load_test_epoch(self, batch_size=50, onehot=False,\n",
    "                        shuffle_within=False, shuffle_paths=False, \n",
    "                        seed=None):\n",
    "        for batch_x, batch_y in self._load_epoch(which='test',\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 onehot=onehot,\n",
    "                                                 shuffle_within=shuffle_within,\n",
    "                                                 shuffle_paths=shuffle_paths,\n",
    "                                                 seed=seed):\n",
    "            yield batch_x, batch_y\n",
    "            \n",
    "    def load_validation_epoch(self, batch_size=50, onehot=False,\n",
    "                         shuffle_within=False, shuffle_paths=False, \n",
    "                         seed=None):\n",
    "        for batch_x, batch_y in self._load_epoch(which='valid',\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 onehot=onehot,\n",
    "                                                 shuffle_within=shuffle_within,\n",
    "                                                 shuffle_paths=shuffle_paths,\n",
    "                                                 seed=seed):\n",
    "            yield batch_x, batch_y\n",
    "\n",
    "    def _load_epoch(self, which='train', batch_size=50, onehot=False,\n",
    "                    shuffle_within=True, shuffle_paths=True, seed=None):\n",
    "\n",
    "        if which == 'train':\n",
    "            paths = self.train_batchpaths\n",
    "        elif which == 'valid':\n",
    "            paths = self.valid_batchpaths\n",
    "        elif which == 'test':\n",
    "            paths = self.test_batchpaths\n",
    "        else:\n",
    "            raise ValueError('`which` must be \"train\" or \"test\". Got %s.' %\n",
    "                             which)\n",
    "            \n",
    "        rgen = np.random.RandomState(seed)\n",
    "        if shuffle_paths:\n",
    "            paths = rgen.shuffle(paths)\n",
    "\n",
    "        for batch in paths:\n",
    "\n",
    "            dct = np.load(batch)\n",
    "\n",
    "            if onehot:\n",
    "                labels = (np.arange(self.n_classes) == \n",
    "                          dct['labels'][:, None]).astype(np.uint8)\n",
    "            else:\n",
    "                labels = dct['labels']\n",
    "\n",
    "            if self.normalize:\n",
    "                # normalize to [0, 1] range\n",
    "                data = dct['data'].astype(np.float32) / 255.\n",
    "            else:\n",
    "                data = dct['data']\n",
    "\n",
    "            arrays = [data, labels]\n",
    "            del dct\n",
    "            indices = np.arange(arrays[0].shape[0])\n",
    "\n",
    "            if shuffle_within:\n",
    "                rgen.shuffle(indices)\n",
    "\n",
    "            for start_idx in range(0, indices.shape[0] - batch_size + 1,\n",
    "                                   batch_size):\n",
    "                index_slice = indices[start_idx:start_idx + batch_size]\n",
    "                yield (ary[index_slice] for ary in arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loader = BatchLoader(minibatches_path='./mnist_npz/', \n",
    "                           normalize=True)\n",
    "\n",
    "for batch_x, batch_y in batch_loader.load_train_epoch(batch_size=50, onehot=True):\n",
    "    print(batch_x.shape)\n",
    "    print(batch_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for batch_x, batch_y in batch_loader.load_train_epoch(batch_size=50, onehot=True):\n",
    "    cnt += batch_x.shape[0]\n",
    "    \n",
    "print('One training epoch contains %d images' % cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch():\n",
    "    for batch_x, batch_y in batch_loader.load_train_epoch(batch_size=50, onehot=True):\n",
    "        pass\n",
    "    \n",
    "% timeit one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training a Model using TensorFlow's feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 256\n",
    "height, width = 28, 28\n",
    "n_classes = 10\n",
    "\n",
    "\n",
    "##########################\n",
    "### GRAPH DEFINITION\n",
    "##########################\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    tf.set_random_seed(123)\n",
    "\n",
    "    # Input data\n",
    "    tf_x = tf.placeholder(tf.float32, [None, height, width, 1], name='features')\n",
    "    tf_x_flat = tf.reshape(tf_x, shape=[-1, height*width])\n",
    "    tf_y = tf.placeholder(tf.int32, [None, n_classes], name='targets')\n",
    "\n",
    "    # Model parameters\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.truncated_normal([width*height, n_hidden_1], stddev=0.1)),\n",
    "        'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=0.1)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes], stddev=0.1))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.zeros([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Multilayer perceptron\n",
    "    layer_1 = tf.add(tf.matmul(tf_x_flat, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "\n",
    "    # Loss and optimizer\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=tf_y)\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(cost, name='train')\n",
    "\n",
    "    # Prediction\n",
    "    correct_prediction = tf.equal(tf.argmax(tf_y, 1), tf.argmax(out_layer, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loader = BatchLoader(minibatches_path='./mnist_npz/', \n",
    "                           normalize=True)\n",
    "\n",
    "# preload small validation set\n",
    "# by unpacking the generator\n",
    "[valid_data] = batch_loader.load_validation_epoch(batch_size=5000, \n",
    "                                                   onehot=True)\n",
    "valid_x, valid_y = valid_data[0], valid_data[1]\n",
    "del valid_data\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "\n",
    "        n_batches = 0\n",
    "        for batch_x, batch_y in batch_loader.load_train_epoch(batch_size=batch_size, \n",
    "                                                              onehot=True, \n",
    "                                                              seed=epoch):\n",
    "            n_batches += 1\n",
    "            _, c = sess.run(['train', 'cost:0'], feed_dict={'features:0': batch_x,\n",
    "                                                            'targets:0': batch_y.astype(np.int)})\n",
    "            avg_cost += c\n",
    "        \n",
    "        train_acc = sess.run('accuracy:0', feed_dict={'features:0': batch_x,\n",
    "                                                      'targets:0': batch_y})\n",
    "        \n",
    "        valid_acc = sess.run('accuracy:0', feed_dict={'features:0': valid_x,\n",
    "                                                      'targets:0': valid_y})  \n",
    "        \n",
    "        print(\"Epoch: %03d | AvgCost: %.3f\" % (epoch + 1, avg_cost / n_batches), end=\"\")\n",
    "        print(\" | MbTrain/Valid ACC: %.3f/%.3f\" % (train_acc, valid_acc))\n",
    "        \n",
    "        \n",
    "    # imagine test set is too large to fit into memory:\n",
    "    test_acc, cnt = 0., 0\n",
    "    for test_x, test_y in batch_loader.load_test_epoch(batch_size=100, \n",
    "                                                       onehot=True):   \n",
    "        cnt += 1\n",
    "        acc = sess.run(accuracy, feed_dict={'features:0': test_x,\n",
    "                                            'targets:0': test_y})\n",
    "        test_acc += acc\n",
    "    print('Test ACC: %.3f' % (test_acc / cnt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
